(50000L, 784L)
compiled score function
garray(144.1488494873047)
Compiled loss functions
0 0.0 144.935302734
1 0.0 144.837036133
2 0.0 144.78477478
3 0.0 144.675247192
4 0.0 144.479415894
5 0.0 144.399368286
6 0.0 144.186950684
7 0.0 144.364852905
8 0.0 144.106567383
9 0.0 144.125640869
10 0.0 144.28527832
11 0.0 143.999694824
12 0.0 143.95211792
13 0.0 143.99659729
14 0.0 144.062561035
15 0.0 143.93359375
16 0.0 143.788040161
17 0.0 143.697540283
18 0.0 143.702301025
19 0.0 143.822860718
20 0.0 143.547943115
21 0.0 143.759338379
22 0.0 143.690002441
23 0.0 143.674972534
24 0.0 143.698150635
25 0.0 143.727127075
26 0.0 143.510360718
27 0.0 143.523452759
28 0.0 143.492950439
29 0.0 143.581497192
30 0.0 143.403823853
31 0.0 143.618865967
32 0.0 143.447769165
33 0.0 143.556472778
34 0.0 143.388412476
35 0.0 143.328033447
36 0.0 143.882369995
37 0.0 143.331130981
38 0.0 143.558227539
39 0.0 143.321563721
40 0.0 143.54057312
41 0.0 143.36630249
42 0.0 143.36529541
43 0.0 143.37437439
44 0.0 143.388702393
45 0.0 143.205657959
46 0.0 143.205154419
47 0.0 143.363647461
48 0.0 143.471359253
49 0.0 143.322479248
50 0.0 143.548629761
51 0.0 143.167404175
52 0.0 143.308486938
53 0.0 143.211547852
54 0.0 143.164855957
55 0.0 143.30065918
56 0.0 143.284301758
57 0.0 143.185668945
58 0.0 143.431930542
59 0.0 143.098800659
60 0.0 143.076568604
61 0.0 143.238220215
62 0.0 143.213027954
63 0.0 143.074111938
64 0.0 142.990707397
65 0.0 143.255828857
66 0.0 143.089477539
67 0.0 143.273925781
68 0.0 143.200775146
69 0.0 142.937835693
70 0.0 143.191467285
71 0.0 143.105514526
72 0.0 142.958084106
73 0.0 143.02305603
74 0.0 142.872451782
75 0.0 143.156997681
76 0.0 142.907287598
77 0.0 142.933502197
78 0.0 143.140457153
79 0.0 143.082901001
80 0.0 142.862716675
81 0.0 142.939849854
82 0.0 142.892333984
83 0.0 143.016677856
84 0.0 142.905334473
85 0.0 142.859817505
86 0.0 142.857513428
87 0.0 142.839065552
88 0.0 142.765823364
89 0.0 142.791900635
90 0.0 143.061218262
91 0.0 142.719589233
92 0.0 142.897232056
93 0.0 142.765441895
94 0.0 142.800354004
95 0.0 142.900253296
96 0.0 142.884552002
97 0.0 142.785888672
98 0.0 142.990814209
99 0.0 142.924224854
100 0.0 142.661193848
101 0.0 142.688903809
102 0.0 142.915679932
103 0.0 142.796127319
104 0.0 142.651382446
105 0.0 142.802520752
106 0.0 142.800170898
107 0.0 142.635910034
108 0.0 142.801345825
109 0.0 142.873123169
110 0.0 142.675186157
111 0.0 142.692047119
112 0.0 142.736968994
113 0.0 142.429718018
114 0.0 142.751968384
115 0.0 142.702987671
116 0.0 142.719970703
117 0.0 142.631652832
118 0.0 142.791915894
119 0.0 142.633239746
120 0.0 142.682220459
121 0.0 142.56803894
122 0.0 142.538879395
123 0.0 142.554824829
124 0.0 142.548553467
125 0.0 142.892562866
126 0.0 142.793869019
127 0.0 142.699127197
128 0.0 142.512283325
129 0.0 142.668426514
130 0.0 142.510391235
131 0.0 142.541702271
132 0.0 142.554244995
133 0.0 142.660369873
134 0.0 142.402832031
135 0.0 142.503692627
136 0.0 142.671173096
137 0.0 142.734054565
138 0.0 142.597824097
139 0.0 142.513214111
140 0.0 142.554016113
141 0.0 142.463867188
142 0.0 142.625549316
143 0.0 142.447021484
144 0.0 142.347732544
145 0.0 142.782730103
146 0.0 142.444122314
147 0.0 142.310516357
148 0.0 142.647445679
149 0.0 142.671203613
150 0.0 142.506591797
151 0.0 142.49937439
152 0.0 142.333496094
153 0.0 142.481842041
154 0.0 142.471694946
155 0.0 142.576889038
156 0.0 142.445709229
157 0.0 142.791900635
158 0.0 142.442672729
159 0.0 142.439147949
160 0.0 142.669723511
161 0.0 142.497711182
162 0.0 142.611312866
163 0.0 142.505767822
164 0.0 142.51109314
165 0.0 142.479919434
166 0.0 142.268188477
167 0.0 142.384994507
168 0.0 142.714569092
169 0.0 142.265533447
170 0.0 142.462203979
171 0.0 142.423599243
172 0.0 142.393295288
173 0.0 142.311065674
174 0.0 142.613723755
175 0.0 142.343551636
176 0.0 142.457931519
177 0.0 142.221191406
178 0.0 142.519393921
179 0.0 142.422470093
180 0.0 142.504760742
181 0.0 142.241088867
182 0.0 142.542434692
183 0.0 142.140136719
184 0.0 142.271606445
185 0.0 142.282577515
186 0.0 142.418746948
187 0.0 142.472503662
188 0.0 142.367477417
189 0.0 142.176467896
190 0.0 142.372039795
191 0.0 142.048080444
192 0.0 142.372253418
193 0.0 142.271606445
194 0.0 142.349319458
195 0.0 142.246917725
196 0.0 142.058349609
197 0.0 142.2472229
198 0.0 142.10798645
199 0.0 142.326461792
200 0.0 142.268218994
201 0.0 142.280410767
202 0.0 141.962249756
203 0.0 142.355102539
204 0.0 142.162796021
205 0.0 142.08744812
206 0.0 142.342590332
207 0.0 142.267242432
208 0.0 142.338928223
209 0.0 142.095458984
210 0.0 142.422851562
211 0.0 142.193466187
212 0.0 142.093765259
213 0.0 142.247146606
214 0.0 142.276290894
215 0.0 142.071929932
216 0.0 142.031661987
217 0.0 142.078720093
218 0.0 142.338790894
219 0.0 142.020401001
220 0.0 142.016403198
221 0.0 142.124740601
222 0.0 141.873764038
223 0.0 142.196044922
224 0.0 141.983764648
225 0.0 142.072097778
226 0.0 142.023620605
227 0.0 142.225204468
228 0.0 142.072052002
229 0.0 142.122253418
230 0.0 142.185775757
231 0.0 142.019226074
232 0.0 142.23878479
233 0.0 142.099716187
234 0.0 142.157669067
235 0.0 141.977142334
236 0.0 141.972045898
237 0.0 142.243927002
238 0.0 141.922119141
239 0.0 142.600357056
240 0.0 142.096389771
241 0.0 142.207626343
242 0.0 142.079467773
243 0.0 142.076293945
244 0.0 142.051513672
245 0.0 141.995544434
246 0.0 141.9296875
247 0.0 142.381225586
248 0.0 141.983566284
249 0.0 142.202468872
creating pickled numpy output
wrote pickled numpy output
Final validation loss
garray(142.07354736328125)
Final test loss
garray(143.38650512695312)
Theano: Initializing cuBLAS
